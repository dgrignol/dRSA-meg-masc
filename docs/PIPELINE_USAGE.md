# MEG-MASC Analysis Pipeline Guide

This document summarises how to run the MEG-MASC processing scripts shipped with the repository, how to combine them with the provided pipeline wrappers, and how to submit the full workflow to an SGE cluster. Every command below assumes you execute it from the repository root with the desired Python environment activated (e.g. `source .venv/bin/activate`). All scripts expose `--help`; consult it for the definitive option list.

---

## 1. Prerequisites

- **Python environment** – install dependencies with `pip install -r requirements.txt`. Most scripts rely on `mne`, `numpy`, `scipy`, `matplotlib`, and a handful of audio/text libraries.
Locally you can use: 
`cd /Volumes/MORWUR/Projects/DAMIANO/SpeDiction/meg-masc`
`PYTHONPATH=".venv/lib/python3.13/site-packages"`
Then run your script e.g. `python D1_group_cluster_analysis.py --subjects $(seq -w 1 2)` etc...
On the cluster, you activate the micromamba env: `micromamba activate drsa311   # or whichever env has Python 3`
Then run your script.
- **Data layout** – the anonymised BIDS dataset must live in `bids_anonym/`. All derivatives are created inside `derivatives/`, and dRSA outputs are written to `results/`.
- **GloVe embeddings** – scripts that touch word embeddings (B4 and the wrappers) require a plain-text GloVe file (e.g. `glove.6B.300d.txt`). Store its path in `glove_path.txt`, set `$GLOVE_PATH`, or pass `--glove-path`.

---

## 2. Stage-by-stage scripts

### A1_preprocess_data.py
Minimal preprocessing plus sentence masking for every BIDS run.

- **Input**: Raw MEG runs under `bids_anonym/`.
- **Output**: Preprocessed FIF files, sentence masks, audio waveforms, and metadata in `derivatives/preprocessed/sub-XX/ses-*/task-*/`.
- **Key options**
  - `--subjects 01 02 …` – limit processing to specific subjects (default: all available).
  - `--overwrite` – overwrite existing derivatives.
  - `--log-level {DEBUG,INFO,…}` – adjust verbosity.
- **Usage**
  ```bash
  python A1_preprocess_data.py --subjects 02 03 --overwrite
  python A1_preprocess_data.py --log-level DEBUG
  ```

### A2_concatenate_subject_data.py
Concatenates per-run derivatives into subject-level arrays and audio files.

- **Input**: Outputs from A1 in `derivatives/preprocessed`.
- **Output**: `*_concatenated_*` files beneath `derivatives/preprocessed/sub-XX/concatenated/`, plus provenance JSON.
- **Key options**
  - `--derivatives-root PATH` – override the derivatives directory (default `derivatives`).
  - `--subject sub-03` – subject to process (default `sub-01`).
  - `--overwrite` – replace existing concatenated outputs.
- **Usage**
  ```bash
  python A2_concatenate_subject_data.py --subject sub-05 --overwrite
  python A2_concatenate_subject_data.py --derivatives-root /tmp/derivatives --subject 07
  ```

> **Note**: A3 (`A3_resample_concatenated_data.py`) is automatically invoked by both pipeline wrappers to downsample the concatenated files. Run it manually if you skip the wrappers.

### B1_model_envelope.py
Builds broadband gammatone speech envelopes for every run and subject.

- **Input**: Audio + metadata generated by A1.
- **Output**: Envelope arrays in `derivatives/Models/envelope/`, concatenated subject-level envelopes, and optional reports under `derivatives/reports/`.
- **Key options**
  - `--subjects 01 02` – subset to process (default: all).
  - `--overwrite` – regenerate envelopes and reports.
  - `--log-level` – adjust verbosity.
- **Usage**
  ```bash
  python B1_model_envelope.py --subjects 02 03
  python B1_model_envelope.py --overwrite --log-level DEBUG
  ```

### B2_wordfreq.py
Generates word-frequency trajectories aligned to MEG timepoints.

- **Input**: Preprocessed metadata (A1) and BIDS events.
- **Output**: Per-run arrays in `derivatives/Models/wordfreq/` plus optional concatenated and resampled subject-level series.
- **Key options**
  - `--subjects …` – restrict processing.
  - `--overwrite` – replace existing outputs.
  - `--no-concat` – skip subject-level concatenation/resampling.
  - `--target-rate 100` – resample concatenated arrays to a desired rate (Hz).
  - `--plot` / `--plot-max-points` – emit diagnostic plots.
- **Usage**
  ```bash
  python B2_wordfreq.py --subjects 02 03 --target-rate 100 --plot
  python B2_wordfreq.py --overwrite --no-concat
  ```

### B3_voicing.py
Creates phoneme voicing (+1/-1) trajectories aligned with MEG timepoints.

- **Input**: A1 derivatives, BIDS events, and `phoneme_info.csv`.
- **Output**: Per-run voicing arrays (`derivatives/Models/voicing/`), optional concatenated/resampled series, and optional plots.
- **Key options** mirror B2:
  - `--subjects`, `--overwrite`, `--no-concat`, `--target-rate`, `--plot`, `--plot-max-points`, `--log-level`.
- **Usage**
  ```bash
  python B3_voicing.py --subjects 02 03 --target-rate 100 --plot
  python B3_voicing.py --overwrite --no-concat
  ```

### B4_glove.py
Builds subject-level GloVe embedding trajectories on a 100 Hz grid.

- **Input**: Concatenation metadata, BIDS word events, and a GloVe text file.
- **Output**: `*_concatenated_glove_100Hz.npy` (memmap-friendly), metadata JSON, and optional diagnostic plots under `derivatives/Models/glove/`.
- **Key options**
  - `--subjects …` – subset of subjects (default: all).
  - `--glove-path /path/to/glove.6B.300d.txt` – **required** unless configured via `GLOVE_PATH` or `glove_path.txt`.
  - `--target-rate 100` – sampling rate for the output (default 100 Hz).
  - `--overwrite`, `--random-seed`, `--plot`, `--plot-max-points`, `--log-level`.
- **Usage**
  ```bash
  python B4_glove.py --subjects 02 03 --glove-path ~/data/glove.6B.300d.txt
  python B4_glove.py --glove-path ./glove.840B.300d.txt --target-rate 50 --plot
  ```

### C1_dRSA_run.py
Runs the subject-level dynamic RSA analysis.

- **Input**: Concatenated MEG, masks, and all model trajectories (envelope, word frequency, voicing, GloVe).
- **Output** per subject:
  - `results/sub-XX_res100_correlation_dRSA_matrices.npy`
  - `results/sub-XX_res100_correlation_metadata.json`
  - `results/sub-XX_res100_correlation_plot.png`
- **Invocation**: The script expects one positional argument – the subject label or ID.
  ```bash
  python C1_dRSA_run.py sub-02
  python C1_dRSA_run.py 03            # equivalent to sub-03
  ```
- **Tuning**: Edit the constants near the bottom of the file (e.g., `averaging_diagonal_time_window_sec`, `n_subsamples`) to adjust the analysis.

### D1_group_cluster_analysis.py
Aggregates subject dRSA results, performs a cluster-based permutation test, and generates summary figures.

- **Input**: Subject-level outputs from C1 located in `results/`.
- **Output** (one set per neural signal reported in the metadata):
  - `group_dRSA_summary_<NEURAL_LABEL>.png` (raster, 300 dpi)
  - `group_dRSA_summary_<NEURAL_LABEL>.pdf` (vector export for Illustrator)
  - `group_dRSA_summary_<NEURAL_LABEL>.npz` (cache containing all aggregated arrays and settings)

  The `<NEURAL_LABEL>` suffix is derived from `metadata["neural_signal_labels"]` for the first subject. Spaces are converted to underscores (e.g., `MEG Full 1 → MEG_Full_1`). The figure title also includes the full label, so expect separate outputs such as `Group-level dRSA summary | MEG Full 2`. If only a single neural signal is present, the suffix still uses the label to keep filenames explicit.

- **Key options**
  - `--subjects 02 03 …` – subjects to include (e.g., `--subjects $(seq -w 1 27)`).
  - `--models …` – ordered model labels to average; they must match the `selected_model_labels` stored in every subject’s metadata.
  - `--results-dir PATH` – location of individual subject results (default `results`).
  - `--lag-metric`, `--cluster-alpha`, `--permutation-alpha`, `--n-permutations`.
  - `--output PATH` – base path for figures (PNG) and cache (NPZ); the `_NEURAL_LABEL` suffix is appended automatically.
  - `--summary-cache PATH` – override the cache location (defaults to `<output>.npz` before the suffix is added).
  - `--plot-only` – skip recomputing clusters; regenerate figures from the cache.
  - `--log-level`.
- **Usage**
  ```bash
  # Run the full aggregation and permutation test
  python D1_group_cluster_analysis.py \
    --subjects $(seq -w 1 27) \
    --models "Envelope" "Phoneme Voicing" "Word Frequency" "GloVe" "GloVe Norm" \
    --results-dir results \
    --output results/group_level/group_dRSA_summary.png

  # Regenerate figures from cached data (one command per cache if multiple neural signals exist)
  python D1_group_cluster_analysis.py \
    --plot-only \
    --summary-cache results/group_level/group_dRSA_summary_MEG_Full_1.npz \
    --output results/group_level/group_dRSA_summary.png
  ```
  When using multiple neural signals, rerun the `--plot-only` command with each cache file produced during the initial analysis.

---

## 3. Pipeline wrappers

### pipeline_wrapper.py – full pipeline
Runs every step (A1→D1) for the chosen subjects without deleting intermediates.

- **Essential options**
  - `--subjects 2-23 25` – required; supports comma/space-separated IDs and ranges.
  - `--glove-path` – optional if `$GLOVE_PATH` or `glove_path.txt` is configured.
  - `--overwrite` – forwards the flag to individual scripts.
  - `--continue-on-error` – keep running even if a step fails.
  - `--group-subjects` – override the subject list passed to D1.
  - `--results-dir`, `--lag-metric`, `--models`, `--d1-output`, `--d1-n-permutations`.
  - `--log-level`.
- **Examples**
  ```bash
  # Full cohort with explicit GloVe path
  python pipeline_wrapper.py --subjects 2-23 --glove-path /data/glove.6B.300d.txt

  # Rerun a subset, overwriting intermediates and forcing D1 to use custom models
  python pipeline_wrapper.py \
    --subjects 05 06 \
    --overwrite \
    --models Envelope "Word Frequency" GloVe \
    --group-subjects 05 06
  ```

### pipeline_wrapper_low_storage.py – sequential, low-footprint mode
Processes subjects one at a time, pruning bulky intermediates after C1 (native-rate concatenates, run-level FIF files, and redundant model arrays) while keeping the 100 Hz subject-level data needed for future dRSA reruns. After the per-subject loop it runs D1 on the successful subset.

- **Essential options**
  - `--subjects …` (required) – same syntax as the full wrapper.
  - `--glove-path` – required if not configured globally.
  - `--overwrite`, `--continue-on-error`.
  - `--keep-derivatives` – skip cleanup for debugging.
  - `--keep-reports` – keep HTML/PDF reports while still removing large arrays.
  - `--results-dir`, `--models`, `--lag-metric`, `--d1-output`, `--d1-n-permutations`, `--log-level`.
- **Examples**
  ```bash
  python pipeline_wrapper_low_storage.py \
    --subjects 2-23 \
    --glove-path /data/glove.6B.300d.txt \
    --continue-on-error \
    --keep-reports

  # Debug a single subject without deleting any derivatives
  python pipeline_wrapper_low_storage.py \
    --subjects 07 \
    --glove-path /data/glove.6B.300d.txt \
    --keep-derivatives --log-level DEBUG
  ```

---

## 4. Cluster submission scripts

Three helper scripts show how to submit common workloads to SGE:

- `s2_submit_python_wrapper.sh` – legacy full-pipeline submission (kept for reference).
- `s2_submit_python_wrapper_low_storage.sh` – sequential wrapper variant that prunes intermediates after each subject.
- `s2_submit_D1_group.sh` – runs only the group-level dRSA aggregation (D1) once all per-subject outputs exist.

General checklist:

1. **Adjust paths** – update `WD` (repository root on the cluster) and any resource paths such as `GLOVE`.
2. **Environment** – the scripts assume `micromamba activate drsa311`. Swap in your own environment activation if needed.
3. **Resources** – tweak `#$` directives (`-pe`, `-l h_vmem`, runtime limits, etc.) to match your quota and job size.
4. **Command** – edit the final `python …` line(s) with the exact arguments you would use locally (e.g., subject list, models, permutation count).
5. **Submission** – make the script executable (`chmod +x s2_submit_D1_group.sh`) and submit via `qsub`:
   ```bash
   qsub s2_submit_python_wrapper_low_storage.sh    # per-subject loop
   qsub s2_submit_D1_group.sh                      # group aggregation only
   ```

**Example**: to run D1 over subjects 01–27 with the default model set:
```bash
python D1_group_cluster_analysis.py \
  --subjects $(seq -w 1 27) \
  --models "Envelope" "Phoneme Voicing" "Word Frequency" "GloVe" "GloVe Norm" \
  --results-dir results \
  --lag-metric correlation \
  --output results/group_level/group_dRSA_summary.png
```
Mirror that command in `s2_submit_D1_group.sh` before submitting.

Logs from the cluster run are written to `logs/<jobname>.<jobid>.{out,err}` as configured at the top of each script.

---

## 5. Tips and troubleshooting

- Use `--log-level DEBUG` when debugging a stage; most scripts print detailed progress and file paths at that level.
- When rerunning D1 with the same settings, prefer `--plot-only` to skip the (potentially lengthy) permutation test.
- The wrappers propagate failures—check the `results/` directory after a run to verify that every subject produced both the matrix (`*_dRSA_matrices.npy`) and metadata files.
- For ad-hoc experimentation, run individual scripts with `--help` to see defaults and optional features not covered above.

Happy analysing!
