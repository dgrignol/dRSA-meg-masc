# MEG-MASC Analysis Pipeline Guide

This document summarises how to run the MEG-MASC processing scripts shipped with the repository, how to combine them with the provided pipeline wrappers, and how to submit the full workflow to an SGE cluster. Every command below assumes you execute it from the repository root with the desired Python environment activated (e.g. `source .venv/bin/activate`). All scripts expose `--help`; consult it for the definitive option list.

---

## 1. Prerequisites

- **Python environment** – install dependencies with `pip install -r requirements.txt`. Most scripts rely on `mne`, `numpy`, `scipy`, `matplotlib`, and a handful of audio/text libraries.
Locally you can use: 
`cd /Volumes/MORWUR/Projects/DAMIANO/SpeDiction/meg-masc`
`PYTHONPATH=".venv/lib/python3.13/site-packages"`
Then run your script e.g. `python D1_group_cluster_analysis.py --subjects $(seq -w 1 2)` etc...
On the cluster, you activate the micromamba env: `micromamba activate drsa311   # or whichever env has Python 3`
Then run your script.
- **Data layout** – the anonymised BIDS dataset must live in `bids_anonym/`. All derivatives are created inside `derivatives/`, and dRSA outputs are written to `results/`.
- **GloVe embeddings** – scripts that touch word embeddings (B4 and the wrappers) require a plain-text GloVe file (e.g. `glove.6B.300d.txt`). Store its path in `glove_path.txt`, set `$GLOVE_PATH`, or pass `--glove-path`.

---

## 2. Stage-by-stage scripts

### A1_preprocess_data.py
Minimal preprocessing plus sentence masking for every BIDS run.

- **Input**: Raw MEG runs under `bids_anonym/`.
- **Output**: Preprocessed FIF files, sentence masks, audio waveforms, and metadata in `derivatives/preprocessed/sub-XX/ses-*/task-*/`.
- **Key options**
  - `--subjects 01 02 …` – limit processing to specific subjects (default: all available).
  - `--overwrite` – overwrite existing derivatives.
  - `--log-level {DEBUG,INFO,…}` – adjust verbosity.
- **Usage**
  ```bash
  python A1_preprocess_data.py --subjects 02 03 --overwrite
  python A1_preprocess_data.py --log-level DEBUG
  ```

### A2_concatenate_subject_data.py
Concatenates per-run derivatives into subject-level arrays and audio files.

- **Input**: Outputs from A1 in `derivatives/preprocessed`.
- **Output**: `*_concatenated_*` files beneath `derivatives/preprocessed/sub-XX/concatenated/`, plus provenance JSON.
- **Key options**
  - `--derivatives-root PATH` – override the derivatives directory (default `derivatives`).
  - `--subject sub-03` – subject to process (default `sub-01`).
  - `--overwrite` – replace existing concatenated outputs.
- **Usage**
  ```bash
  python A2_concatenate_subject_data.py --subject sub-05 --overwrite
  python A2_concatenate_subject_data.py --derivatives-root /tmp/derivatives --subject 07
  ```

> **Note**: A3 (`A3_resample_concatenated_data.py`) is automatically invoked by both pipeline wrappers to downsample the concatenated files. Run it manually if you skip the wrappers.

### B1_model_envelope.py
Builds broadband gammatone speech envelopes for every run and subject.

- **Input**: Audio + metadata generated by A1.
- **Output**: Envelope arrays in `derivatives/Models/envelope/`, concatenated subject-level envelopes, and optional reports under `derivatives/reports/`.
- **Key options**
  - `--subjects 01 02` – subset to process (default: all).
  - `--overwrite` – regenerate envelopes and reports.
  - `--log-level` – adjust verbosity.
- **Usage**
  ```bash
  python B1_model_envelope.py --subjects 02 03
  python B1_model_envelope.py --overwrite --log-level DEBUG
  ```

### B2_wordfreq.py
Generates word-frequency trajectories aligned to MEG timepoints.

- **Input**: Preprocessed metadata (A1) and BIDS events.
- **Output**: Per-run arrays in `derivatives/Models/wordfreq/` plus optional concatenated and resampled subject-level series.
- **Key options**
  - `--subjects …` – restrict processing.
  - `--overwrite` – replace existing outputs.
  - `--no-concat` – skip subject-level concatenation/resampling.
  - `--target-rate 100` – resample concatenated arrays to a desired rate (Hz).
  - `--plot` / `--plot-max-points` – emit diagnostic plots.
- **Usage**
  ```bash
  python B2_wordfreq.py --subjects 02 03 --target-rate 100 --plot
  python B2_wordfreq.py --overwrite --no-concat
  ```

### B3_voicing.py
Creates phoneme voicing (+1/-1) trajectories aligned with MEG timepoints.

- **Input**: A1 derivatives, BIDS events, and `phoneme_info.csv`.
- **Output**: Per-run voicing arrays (`derivatives/Models/voicing/`), optional concatenated/resampled series, and optional plots.
- **Key options** mirror B2:
  - `--subjects`, `--overwrite`, `--no-concat`, `--target-rate`, `--plot`, `--plot-max-points`, `--log-level`.
- **Usage**
  ```bash
  python B3_voicing.py --subjects 02 03 --target-rate 100 --plot
  python B3_voicing.py --overwrite --no-concat
  ```

### B4_glove.py
Builds subject-level GloVe embedding trajectories on a 100 Hz grid.

- **Input**: Concatenation metadata, BIDS word events, and a GloVe text file.
- **Output**: `*_concatenated_glove_100Hz.npy` (memmap-friendly), metadata JSON, and optional diagnostic plots under `derivatives/Models/glove/`.
- **Key options**
  - `--subjects …` – subset of subjects (default: all).
  - `--glove-path /path/to/glove.6B.300d.txt` – **required** unless configured via `GLOVE_PATH` or `glove_path.txt`.
  - `--target-rate 100` – sampling rate for the output (default 100 Hz).
  - `--overwrite`, `--random-seed`, `--plot`, `--plot-max-points`, `--log-level`.
- **Usage**
  ```bash
  python B4_glove.py --subjects 02 03 --glove-path ~/data/glove.6B.300d.txt
  python B4_glove.py --glove-path ./glove.840B.300d.txt --target-rate 50 --plot
  ```

### C1_dRSA_run.py
Runs the subject-level dynamic RSA analysis.

- **Input**: Concatenated MEG, masks, and all model trajectories (envelope, word frequency, voicing, GloVe).
- **Output** per subject:
  - `results/sub-XX_res100_correlation_dRSA_matrices.npy`
  - `results/sub-XX_res100_correlation_metadata.json`
  - `results/sub-XX_res100_correlation_plot.png`
- **Invocation**: The script expects one positional argument – the subject label or ID.
  ```bash
  python C1_dRSA_run.py sub-02
  python C1_dRSA_run.py 03            # equivalent to sub-03
  ```
- **Tuning**: Edit the constants near the bottom of the file (e.g., `averaging_diagonal_time_window_sec`, `n_subsamples`) to adjust the analysis.

### D1_group_cluster_analysis.py
Aggregates subject dRSA results, performs a cluster-based permutation test, and generates summary figures.

- **Input**: Subject-level outputs from C1 located in `results/`.
- **Output** (by default in `results/group_level/`):
  - `group_dRSA_summary.png` (raster, 300 dpi)
  - `group_dRSA_summary.pdf` (vector export for Illustrator)
  - `group_dRSA_summary.npz` (cache containing all aggregated arrays and settings)
- **Key options**
  - `--subjects 02 03 …` – subjects to include (e.g., `--subjects 02-23` via shell expansion or the wrappers).
  - `--models …` – ordered model labels to average (default: `Envelope`, `Word Frequency`, `GloVe`, `GloVe Norm`).
  - `--results-dir PATH` – location of individual subject results (default `results`).
  - `--lag-metric`, `--cluster-alpha`, `--permutation-alpha`, `--n-permutations`.
  - `--output PATH` – base path for figures (PNG) and cache (NPZ). A PDF is emitted automatically alongside the PNG.
  - `--summary-cache PATH` – override the cache location (defaults to `<output>.npz`).
  - `--plot-only` – skip recomputing clusters; regenerate figures from the cache.
  - `--log-level`.
- **Usage**
  ```bash
  # Run the full aggregation and permutation test
  python D1_group_cluster_analysis.py \
    --subjects 02 03 04 \
    --models Envelope "Word Frequency" GloVe "GloVe Norm" \
    --results-dir results \
    --output results/group_level/group_dRSA_summary.png

  # Regenerate figures from cached data
  python D1_group_cluster_analysis.py \
    --plot-only \
    --summary-cache results/group_level/group_dRSA_summary.npz \
    --output results/group_level/group_dRSA_summary.png
  ```

---

## 3. Pipeline wrappers

### pipeline_wrapper.py – full pipeline
Runs every step (A1→D1) for the chosen subjects without deleting intermediates.

- **Essential options**
  - `--subjects 2-23 25` – required; supports comma/space-separated IDs and ranges.
  - `--glove-path` – optional if `$GLOVE_PATH` or `glove_path.txt` is configured.
  - `--overwrite` – forwards the flag to individual scripts.
  - `--continue-on-error` – keep running even if a step fails.
  - `--group-subjects` – override the subject list passed to D1.
  - `--results-dir`, `--lag-metric`, `--models`, `--d1-output`, `--d1-n-permutations`.
  - `--log-level`.
- **Examples**
  ```bash
  # Full cohort with explicit GloVe path
  python pipeline_wrapper.py --subjects 2-23 --glove-path /data/glove.6B.300d.txt

  # Rerun a subset, overwriting intermediates and forcing D1 to use custom models
  python pipeline_wrapper.py \
    --subjects 05 06 \
    --overwrite \
    --models Envelope "Word Frequency" GloVe \
    --group-subjects 05 06
  ```

### pipeline_wrapper_low_storage.py – sequential, low-footprint mode
Processes subjects one at a time, pruning bulky intermediates after C1 (native-rate concatenates, run-level FIF files, and redundant model arrays) while keeping the 100 Hz subject-level data needed for future dRSA reruns. After the per-subject loop it runs D1 on the successful subset.

- **Essential options**
  - `--subjects …` (required) – same syntax as the full wrapper.
  - `--glove-path` – required if not configured globally.
  - `--overwrite`, `--continue-on-error`.
  - `--keep-derivatives` – skip cleanup for debugging.
  - `--keep-reports` – keep HTML/PDF reports while still removing large arrays.
  - `--results-dir`, `--models`, `--lag-metric`, `--d1-output`, `--d1-n-permutations`, `--log-level`.
- **Examples**
  ```bash
  python pipeline_wrapper_low_storage.py \
    --subjects 2-23 \
    --glove-path /data/glove.6B.300d.txt \
    --continue-on-error \
    --keep-reports

  # Debug a single subject without deleting any derivatives
  python pipeline_wrapper_low_storage.py \
    --subjects 07 \
    --glove-path /data/glove.6B.300d.txt \
    --keep-derivatives --log-level DEBUG
  ```

---

## 4. Cluster submission scripts

Two helper scripts (`s2_submit_python_wrapper.sh` OUTDATED and `s2_submit_python_wrapper_low_storage.sh`) illustrate how to launch the pipeline on an SGE cluster.

1. **Adjust paths** – edit `WD` to point at the repository root on the cluster file system, and update `GLOVE` if needed.
2. **Environment** – both scripts activate a `micromamba` environment named `drsa311`. Replace the activation block if you use a different manager or env name.
3. **Resources** – tweak the `#$` directives (`-pe`, `-l h_vmem`, etc.) to match your cluster quota.
4. **Command** – the last block invokes either `pipeline_wrapper.py` (OUTDATED) or `pipeline_wrapper_low_storage.py`. Modify arguments just as you would when running locally.
5. **Submission** – make the script executable (`chmod +x s2_submit_python_wrapper.sh`) and submit it:
   ```bash
   qsub s2_submit_python_wrapper.sh
   qsub s2_submit_python_wrapper_low_storage.sh
   ```
- **Example customisation** – to run only subjects 10–12 with the low-storage mode and keep derivatives:
  ```bash
  python pipeline_wrapper_low_storage.py \
    --subjects 10-12 \
    --glove-path "$GLOVE" \
    --keep-derivatives
  ```
  Edit the corresponding lines in the submission script before calling `qsub`.

Logs from the cluster run are written to `logs/<jobname>.<jobid>.{out,err}` as configured at the top of each script.

---

## 5. Tips and troubleshooting

- Use `--log-level DEBUG` when debugging a stage; most scripts print detailed progress and file paths at that level.
- When rerunning D1 with the same settings, prefer `--plot-only` to skip the (potentially lengthy) permutation test.
- The wrappers propagate failures—check the `results/` directory after a run to verify that every subject produced both the matrix (`*_dRSA_matrices.npy`) and metadata files.
- For ad-hoc experimentation, run individual scripts with `--help` to see defaults and optional features not covered above.

Happy analysing!
