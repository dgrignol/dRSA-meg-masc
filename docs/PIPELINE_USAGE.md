# MEG-MASC dRSA Analysis Pipeline Guide

This document summarises how to run the MEG-MASC processing scripts (from preprocessing to dRSA), how to combine them with the pipeline wrappers, and how to submit the full workflow to an SGE cluster. Every command below assumes you execute it from the repository root with the desired Python environment activated (e.g. `source .venv/bin/activate`). All scripts expose `--help`; consult it for the definitive option list.

---

## 1. Prerequisites

- **Python environment** – install dependencies with `pip install -r requirements.txt`. Most scripts rely on `mne`, `numpy`, `scipy`, `matplotlib`, and a handful of audio/text libraries. Activate the local virtual environment (`source .venv/bin/activate`) before running any script, or call its interpreter explicitly (e.g. `./.venv/bin/python D1_group_cluster_analysis.py --subjects $(seq -w 1 2)`). No manual `PYTHONPATH` tweaks are required. On the cluster, activate the micromamba env: `micromamba activate drsa311  # or whichever env has Python 3`, then run your script.
- **Data layout** – the anonymised BIDS dataset must live in `bids_anonym/`. All derivatives are created inside `derivatives/`, and dRSA outputs are stored in named subfolders under `results/<analysis_name>/` (`single_subjects/` and `group_level/`).
- **GloVe embeddings** – scripts that touch word embeddings (B4 and the wrappers) require a plain-text GloVe file (e.g. `glove.6B.300d.txt`). Store its path in `glove_path.txt`, set `$GLOVE_PATH`, or pass `--glove-path`.

---

## 2. Stage-by-stage scripts

### A1_preprocess_data.py
Minimal preprocessing plus sentence masking for every BIDS run.

- **Input**: Raw MEG runs under `bids_anonym/`.
- **Output**: Preprocessed FIF files, sentence masks, audio waveforms, and metadata in `derivatives/preprocessed/sub-XX/ses-*/task-*/`.
- **Key options**
  - `--subjects 01 02 …` – limit processing to specific subjects (default: all available).
  - `--overwrite` – overwrite existing derivatives.
  - `--log-level {DEBUG,INFO,…}` – adjust verbosity.
- **Usage**
  ```bash
  python A1_preprocess_data.py --subjects 02 03 --overwrite
  python A1_preprocess_data.py --log-level DEBUG
  ```

### A2_concatenate_subject_data.py
Concatenates per-run derivatives into subject-level arrays and audio files.

- **Input**: Outputs from A1 in `derivatives/preprocessed`.
- **Output**: `*_concatenated_*` files beneath `derivatives/preprocessed/sub-XX/concatenated/`, plus provenance JSON.
- **Key options**
  - `--derivatives-root PATH` – override the derivatives directory (default `derivatives`).
  - `--subject sub-03` – subject to process (default `sub-01`).
  - `--overwrite` – replace existing concatenated outputs.
- **Usage**
  ```bash
  python A2_concatenate_subject_data.py --subject sub-05 --overwrite
  python A2_concatenate_subject_data.py --derivatives-root /tmp/derivatives --subject 07
  ```

> **Note**: A3 (`A3_resample_concatenated_data.py`) is automatically invoked by both pipeline wrappers to downsample the concatenated files. Run it manually if you skip the wrappers.

### check_word_onset_extraction.py
Quick visual check that the concatenated word onsets exported by A2 align with the audio envelope and sentence mask. 

- **Input**:
`derivatives/preprocessed/<subject>/concatenated/<subject>_concatenated_word_onsets_sec.npy` (from A2_concatenate_subject_data.py), `*_concatenated_sentence_mask_100Hz.npy` (from A2_concatenate_subject_data.py), and the 100 Hz speech envelope at `derivatives/Models/envelope/<subject>/concatenated/<subject>_concatenated_envelope_100Hz.npy` (from B1_model_envelope.py).
- **Output**: `word_onsets.png` in the current directory showing:
  - mean envelope (blue),
  - shaded regions where the sentence mask is active,
  - vertical markers at every extracted word onset.
- **Usage**
  ```bash
  python check_word_onset_extraction.py          # defaults to sub-01 and saves word_onsets.png
  python check_word_onset_extraction.py --help   # edit the script to point at other subjects
  ```
- Adjust `subject` and `time_window` at the top of the script to explore different participants or time spans.


### B1_model_envelope.py
Builds broadband gammatone speech envelopes for every run and subject.

- **Input**: Audio + metadata generated by A1.
- **Output**: Envelope arrays in `derivatives/Models/envelope/`, concatenated subject-level envelopes, and optional reports under `derivatives/reports/`.
- **Key options**
  - `--subjects 01 02` – subset to process (default: all).
  - `--overwrite` – regenerate envelopes and reports.
  - `--log-level` – adjust verbosity.
- **Usage**
  ```bash
  python B1_model_envelope.py --subjects 02 03
  python B1_model_envelope.py --overwrite --log-level DEBUG
  ```

### B2_wordfreq.py
Generates word-frequency trajectories aligned to MEG timepoints.

- **Input**: Preprocessed metadata (A1) and BIDS events.
- **Output**: Per-run arrays in `derivatives/Models/wordfreq/` plus optional concatenated and resampled subject-level series.
- **Key options**
  - `--subjects …` – restrict processing.
  - `--overwrite` – replace existing outputs.
  - `--no-concat` – skip subject-level concatenation/resampling.
  - `--target-rate 100` – resample concatenated arrays to a desired rate (Hz).
  - `--plot` / `--plot-max-points` – emit diagnostic plots.
- **Usage**
  ```bash
  python B2_wordfreq.py --subjects 02 03 --target-rate 100 --plot
  python B2_wordfreq.py --overwrite --no-concat
  ```

### B3_voicing.py
Creates phoneme voicing (+1/-1) trajectories aligned with MEG timepoints.

- **Input**: A1 derivatives, BIDS events, and `phoneme_info.csv`.
- **Output**: Per-run voicing arrays (`derivatives/Models/voicing/`), optional concatenated/resampled series, and optional plots.
- **Key options** mirror B2:
  - `--subjects`, `--overwrite`, `--no-concat`, `--target-rate`, `--plot`, `--plot-max-points`, `--log-level`.
- **Usage**
  ```bash
  python B3_voicing.py --subjects 02 03 --target-rate 100 --plot
  python B3_voicing.py --overwrite --no-concat
  ```

### B4_glove.py
Builds subject-level GloVe embedding trajectories on a 100 Hz grid.

- **Input**: Concatenation metadata, BIDS word events, and a GloVe text file.
- **Output**: `*_concatenated_glove_100Hz.npy` (memmap-friendly), metadata JSON, and optional diagnostic plots under `derivatives/Models/glove/`.
- **Key options**
  - `--subjects …` – subset of subjects (default: all).
  - `--glove-path /path/to/glove.6B.300d.txt` – **required** unless configured via `GLOVE_PATH` or `glove_path.txt`.
  - `--target-rate 100` – sampling rate for the output (default 100 Hz).
  - `--overwrite`, `--random-seed`, `--plot`, `--plot-max-points`, `--log-level`.
- **Usage**
  ```bash
  python B4_glove.py --subjects 02 03 --glove-path ~/data/glove.6B.300d.txt
  python B4_glove.py --glove-path ./glove.840B.300d.txt --target-rate 50 --plot
  ```

### C1_dRSA_run.py
Runs the subject-level dynamic RSA analysis.

- **Input**: Concatenated MEG, masks, and all model trajectories (envelope, word frequency, voicing, GloVe).
- **Output** per subject (inside `results/<analysis_name>/single_subjects/`):
  - `sub-XX_res100_correlation_dRSA_matrices.npy`
  - `sub-XX_res100_correlation_metadata.json`
  - `sub-XX_res100_correlation_plot.png` (target path recorded in the metadata for C2)
  - `cache/subsamples/subsamples_<HASH>.png` (QC plot saved with the cached indices; word onsets appear as red overlays when available)
- **Key options**
  - `subject` – positional subject label or integer.
  - `--analysis-name NAME` – recommended; names the analysis folder under `results/` (default: timestamp such as `20240130_143210`).
  - `--results-root PATH` – parent directory that will contain the analysis folder (default: `results`).
  - `--lock-subsample-to-word-onset` – restrict subsample starts to the concatenated word onset timestamps (requires the numpy file written by A2).
  - `--allow-overlap` – allow subsample windows to overlap (handy when the onset density is low; compatible with and without locking).
- **Usage**
  ```bash
  # Preferred: explicit analysis name reused across scripts
  python C1_dRSA_run.py sub-02 --analysis-name lexical_mem_2024

  # Quick run: auto-generate a timestamped folder under results/
  python C1_dRSA_run.py 03
  ```
- **Word onset workflows**: When `derivatives/preprocessed/<subject>/concatenated/<subject>_concatenated_word_onsets_sec.npy` exists, C1 hashes the timestamps for caching, records their provenance inside the metadata JSON, and overlays them on the subsampling QC figure. Passing `--lock-subsample-to-word-onset` enforces that every subsample starts at a word onset (after trimming windows that would overrun the recording). Toggling either `--lock-subsample-to-word-onset` or `--allow-overlap` automatically invalidates the subsample cache so reruns remain reproducible.
- **Tuning**: Edit the constants near the bottom of the file (e.g., `averaging_diagonal_time_window_sec`, `n_subsamples`) to adjust the analysis.

### C2_plot_dRSA.py
Recreates subject-level dRSA figures from cached matrices and lag curves using the same layout as the group-level plots.

- **Input**: The metadata JSON plus the `*_dRSA_matrices.npy` and optional `*_lag_curves.npy` generated by C1 (typically under `results/<analysis_name>/single_subjects/`).
- **Output**: One 2×N subplot figure per neural signal (PNG, 300 dpi) saved next to the matrices. If multiple neural signals are present, filenames receive a `_NEURAL_LABEL` suffix.
- **Key options**
  - `subject` (positional) – subject label (`sub-02`) or integer (`2`).
  - `--analysis-name NAME` – analysis folder to read from (defaults to the latest available run under `--results-root`).
  - `--results-root PATH` – parent directory containing named analyses (default `results`).
  - `--analysis-id` – override the subject/resolution/method trio with an explicit run ID (bypasses `--analysis-name`).
  - `--resolution`, `--rsa-method` – only needed when reconstructing legacy layouts or using `--analysis-id`.
  - `--results-dir` – legacy override pointing directly to subject outputs.
- **Usage**
  ```bash
  # Preferred: reuse the analysis name chosen for C1
  python C2_plot_dRSA.py sub-01 --analysis-name lexical_mem_2024

  # Quick check: fall back to the most recent analysis automatically
  python C2_plot_dRSA.py sub-01
  ```
- **Notes**
  - When lag bootstrap samples are available, the script overlays the mean lag curve with a confidence band.
  - Each figure mirrors the styling of `D1_group_cluster_analysis.py`, making per-subject and group-level outputs visually consistent.

### D1_group_cluster_analysis.py
Aggregates subject dRSA results, performs a cluster-based permutation test, and generates summary figures.

- **Input**: Subject-level outputs from C1 located in `results/<analysis_name>/single_subjects/`.
- **Output** (one set per neural signal reported in the metadata) written to `results/<analysis_name>/group_level/`:
  - `group_dRSA_summary_<NEURAL_LABEL>.png` (raster, 300 dpi)
  - `group_dRSA_summary_<NEURAL_LABEL>.pdf` (vector export for Illustrator)
  - `group_dRSA_summary_<NEURAL_LABEL>.npz` (cache containing all aggregated arrays and settings)
- **Key options**
  - `--analysis-name NAME` – analysis folder to read/write (default: latest analysis under `--results-root`).
  - `--results-root PATH` – parent directory containing analyses (default `results`).
  - `--subjects`, `--models`, `--lag-metric`, `--cluster-alpha`, `--permutation-alpha`, `--n-permutations` – unchanged.
  - `--force-matrix-clusters` – force matrix-level cluster permutation even when subsamples are not word-onset locked (auto-enabled when metadata reports `lock_subsample_to_word_onset=True`).
  - `--matrix-downsample-factor INT` – average matrices over INT×INT blocks before the permutation test (default 1, i.e. no downsampling).
  - `--results-dir` – legacy override pointing directly to subject-level outputs.
- **Usage**
  ```bash
  # Preferred: explicit analysis
  python D1_group_cluster_analysis.py \
    --analysis-name lexical_mem_2024 \
    --subjects $(seq -w 1 27) \
    --models "Envelope" "Phoneme Voicing" "Word Frequency" "GloVe" "GloVe Norm"

  # Default: reuse the most recent analysis automatically
  python D1_group_cluster_analysis.py --subjects $(seq -w 1 10) --models Envelope "GloVe"

  # Force matrix-level clusters even for unlocked subsamples
  python D1_group_cluster_analysis.py \
    --analysis-name lexical_mem_2024 \
    --subjects $(seq -w 1 27) \
    --models "Envelope" "GloVe" \
    --force-matrix-clusters
  ```

  The `<NEURAL_LABEL>` suffix is derived from `metadata["neural_signal_labels"]` for the first subject. Spaces are converted to underscores (e.g., `MEG Full 1 → MEG_Full_1`). The figure title also includes the full label, so expect separate outputs such as `Group-level dRSA summary | MEG Full 2`. If only a single neural signal is present, the suffix still uses the label to keep filenames explicit.
  When using multiple neural signals, rerun the `--plot-only` command with each cache file produced during the initial analysis.

- **Matrix-level cluster analysis**
  - When subject metadata indicates `lock_subsample_to_word_onset=True`, the script performs 2D cluster permutation tests on the group dRSA matrices in addition to the lag curves. Significant matrix clusters are outlined in white on the heatmaps and stored in the `.npz` cache.
  - Use `--force-matrix-clusters` to run the matrix permutation even if the metadata shows unlocked subsamples (default off). The `--n-permutations` value applies to both lag and matrix tests.
  - `--matrix-downsample-factor` reduces matrix resolution before cluster testing (e.g., `--matrix-downsample-factor 2` averages 2×2 blocks for ~75% faster permutations; `--matrix-downsample-factor 4` averages 4×4 blocks for ~94% faster permutations; larger factors scale roughly with 1/factor²). The resulting significant blocks are projected back onto the full-resolution heatmap for plotting.
  - Matrix permutation results are cached alongside the usual averages so replotting with `--plot-only` reproduces the overlays without recomputation.
- **Logging**
  - Default verbosity is now `DEBUG`, which prints progress messages at the 10th permutation and every subsequent 100 iterations for both lag and matrix cluster loops. Switch to `--log-level INFO` (or higher) to suppress detailed progress reporting.

---

## 3. Pipeline wrappers

### pipeline_wrapper.py – full pipeline
Runs every step (A1→D1) for the chosen subjects without deleting intermediates.

- **Essential options**
  - `--subjects 2-23 25` – required; supports comma/space-separated IDs and ranges.
  - `--glove-path` – optional if `$GLOVE_PATH` or `glove_path.txt` is configured.
  - `--overwrite` – forwards the flag to individual scripts.
  - `--continue-on-error` – keep running even if a step fails.
  - `--group-subjects` – override the subject list passed to D1.
  - `--analysis-name NAME` – recommended; names the results folder reused by C1/D1 (default: timestamp).
  - `--lock-subsample-to-word-onset`, `--allow-overlap` – forward the subsampling controls to C1 when set.
  - `--results-root PATH`, `--lag-metric`, `--models`, `--d1-output`, `--d1-n-permutations`.
  - `--log-level`.
- **Examples**
  ```bash
  # Full cohort with explicit GloVe path
  python pipeline_wrapper.py --subjects 2-23 --glove-path /data/glove.6B.300d.txt --analysis-name lexical_mem_2024

  # Rerun a subset, overwriting intermediates and forcing D1 to use custom models
  python pipeline_wrapper.py \
    --subjects 05 06 \
    --overwrite \
    --analysis-name lexical_mem_debug \
    --models Envelope "Word Frequency" GloVe \
    --group-subjects 05 06
  ```

### pipeline_wrapper_low_storage.py – sequential, low-footprint mode
Processes subjects one at a time, pruning bulky intermediates after C1 (native-rate concatenates, run-level FIF files, and redundant model arrays) while keeping the 100 Hz subject-level data needed for future dRSA reruns. After the per-subject loop it runs D1 on the successful subset.

- **Essential options**
  - `--subjects …` (required) – same syntax as the full wrapper.
  - `--glove-path` – required if not configured globally.
  - `--overwrite`, `--continue-on-error`.
  - `--keep-derivatives` – skip cleanup for debugging.
  - `--keep-reports` – keep HTML/PDF reports while still removing large arrays.
  - `--analysis-name NAME` – recommended; ensures every subject writes into the same results folder (default: timestamp).
  - `--lock-subsample-to-word-onset`, `--allow-overlap` – forward onset locking/overlap flags to C1 before pruning intermediates.
  - `--results-root`, `--models`, `--lag-metric`, `--d1-output`, `--d1-n-permutations`, `--log-level`.
- **Examples**
  ```bash
  python pipeline_wrapper_low_storage.py \
    --subjects 2-23 \
    --glove-path /data/glove.6B.300d.txt \
    --analysis-name lexical_mem_2024 \
    --continue-on-error \
    --keep-reports

  # Debug a single subject without deleting any derivatives
  python pipeline_wrapper_low_storage.py \
    --subjects 07 \
    --glove-path /data/glove.6B.300d.txt \
    --analysis-name lexical_mem_debug \
    --keep-derivatives --log-level DEBUG
  ```

---

## 4. Cluster submission scripts

Eight helper scripts show how to submit common workloads to SGE:

- `s2_submit_A1_preproc.sh` – array job that assigns one subject per task for A1 preprocessing.
- `s2_submit_A2_concat.sh` – array job that runs A2 concatenation subject by subject.
- `s2_submit_A3_resample.sh` – array job that resamples concatenated derivatives (A3).
- `s2_submit_C1_subject.sh` – array job that runs the C1 dRSA analysis per subject.
- `s2_submit_C2_plot.sh` – single job that renders the group/subject plots (C2).
- `s2_submit_python_wrapper.sh` – legacy full-pipeline submission (kept for reference).
- `s2_submit_python_wrapper_low_storage.sh` – sequential wrapper variant that prunes intermediates after each subject.
- `s2_submit_D1_group.sh` – runs only the group-level dRSA aggregation (D1) once all per-subject outputs exist.

General checklist:

1. **Adjust paths** – update `WD` (repository root on the cluster) and any resource paths such as `GLOVE`.
2. **Environment** – the scripts assume `micromamba activate drsa311`. Swap in your own environment activation if needed.
3. **Resources** – tweak `#$` directives (`-pe`, `-l h_vmem`, runtime limits, etc.) to match your quota and job size.
4. **Command** – edit the final `python …` line(s) with the exact arguments you would use locally (e.g., subject list, models, permutation count).
5. **Submission** – make the script executable (e.g., `chmod +x s2_submit_A1_preproc.sh`) and submit via `qsub`:
   ```bash
   qsub s2_submit_A1_preproc.sh              # subject-wise preprocessing
   qsub s2_submit_A2_concat.sh               # subject-wise concatenation
   qsub s2_submit_A3_resample.sh             # subject-wise resampling (A3)
   qsub s2_submit_C1_subject.sh              # subject-wise dRSA (C1)
   qsub s2_submit_C2_plot.sh                 # generate plots (C2)
   qsub s2_submit_python_wrapper_low_storage.sh    # per-subject loop
   qsub s2_submit_D1_group.sh                      # group aggregation only
   ```
   Except for `s2_submit_D1_group.sh`, each helper tolerates an optional leading `--` before additional script arguments so you can keep `qsub` flags separate. For `s2_submit_D1_group.sh` pass extra options directly (no separator) so they reach `D1_group_cluster_analysis.py`.

### s2_submit_A1_preproc.sh – preprocessing array job
Runs `A1_preprocess_data.py` with the subject ID derived from `SGE_TASK_ID`. The script ships with `#$ -t 1-27`, so submitting without extra arguments spawns one task per subject (01–27). Override the range at submit time to target specific participants.

```bash
qsub s2_submit_A1_preproc.sh           # run all subjects (1–27)
qsub -t 2 s2_submit_A1_preproc.sh      # only subject 02
qsub -t 10-15 s2_submit_A1_preproc.sh  # subjects 10–15
```

Each task writes logs to `logs/A1_preproc.<jobid>.<taskid>.{out,err}`.

### s2_submit_A2_concat.sh – concatenation array job
Runs `A2_concatenate_subject_data.py` subject by subject using the same array pattern as the preprocessing script.

```bash
qsub s2_submit_A2_concat.sh           # run concatenation for subjects 01–27
qsub -t 2 s2_submit_A2_concat.sh      # only subject 02
qsub -t 5,7,9 s2_submit_A2_concat.sh  # run a sparse set of subjects
```

Logs are stored per task in `logs/A2_concat.<jobid>.<taskid>.{out,err}`.

### s2_submit_A3_resample.sh – resampling array job
Runs `A3_resample_concatenated_data.py` to downsample concatenated MEG/mask files per subject. Pass extra options (e.g., `--target-rate`) after an optional separating `--`.

```bash
qsub s2_submit_A3_resample.sh                               # subjects 01–27
qsub -t 2 s2_submit_A3_resample.sh                          # only subject 02
qsub -t 5-9 s2_submit_A3_resample.sh -- --target-rate 200   # custom output rate
```

Outputs inherit the base script’s naming (`*_concatenated_meg_<suffix>.npy`) beneath each subject’s `derivatives/preprocessed/.../concatenated/` directory.

### s2_submit_C1_subject.sh – subject-level dRSA array job
Runs `C1_dRSA_run.py` for each subject listed in the array range. The helper appends `--analysis-name` automatically, sourcing the value from `ANALYSIS_NAME` (if set) or falling back to `drsa_$JOB_ID`. Extra CLI arguments passed after an optional separating `--` (to keep `qsub` happy) are forwarded to the Python script.

```bash
qsub s2_submit_C1_subject.sh                         # run subjects 01–27 (defaults to drsa_$JOB_ID)
qsub -t 2 s2_submit_C1_subject.sh                    # only subject 02
qsub -t 5-9 s2_submit_C1_subject.sh -- --analysis-name lexical_mem_2024   # explicit folder
qsub s2_submit_C1_subject.sh -- --lock-subsample-to-word-onset --allow-overlap # extra flags
qsub s2_submit_C1_subject.sh -- --analysis-name lock_to_onset_overlap --lock-subsample-to-word-onset --allow-overlap
```

Logs land in `logs/C1_dRSA.<jobid>.<taskid>.{out,err}`. When passing optional flags remember to include the separator if you supply additional `qsub` options:

```bash
qsub -t 5-9 s2_submit_C1_subject.sh -- --lock-subsample-to-word-onset --allow-overlap
```

### s2_submit_C2_plot.sh – dRSA plotting array job
Runs `C2_plot_dRSA.py` per subject using the array index to derive the label. If neither `--analysis-name` nor `$ANALYSIS_NAME` is provided, the helper resolves the latest analysis under `results/`. Pass additional plotting options after an optional `--`.

```bash
qsub s2_submit_C2_plot.sh                     # render plots for subjects 01–27 (latest analysis)
qsub -t 2 s2_submit_C2_plot.sh                # only subject 02
qsub -t 5-9 s2_submit_C2_plot.sh -- --analysis-name lexical_mem_2024
```

Logs are stored in `logs/C2_plot.<jobid>.<taskid>.{out,err}`.

**Example**: to run D1 over subjects 01–27 with the default model set:
```bash
python D1_group_cluster_analysis.py \
  --analysis-name lexical_mem_2024 \
  --subjects $(seq -w 1 27) \
  --models "Envelope" "Phoneme Voicing" "Word Frequency" "GloVe" "GloVe Norm" \
  --lag-metric correlation
```
Mirror that command in `s2_submit_D1_group.sh` before submitting.

Logs from the cluster run are written to `logs/<jobname>.<jobid>.{out,err}` as configured at the top of each script.

---

## 5. Tips and troubleshooting

- Use `--log-level DEBUG` when debugging a stage; most scripts print detailed progress and file paths at that level.
- When rerunning D1 with the same settings, prefer `--plot-only` to skip the (potentially lengthy) permutation test.
- The wrappers propagate failures—check the `results/` directory after a run to verify that every subject produced both the matrix (`*_dRSA_matrices.npy`) and metadata files.
- For ad-hoc experimentation, run individual scripts with `--help` to see defaults and optional features not covered above.

---

## Summary Workflow (A1 → D1)

Use this checklist to drive the entire pipeline with consistent analysis folders. Commands are shown for direct execution and, when available, the corresponding SGE helpers. Replace subject lists, model choices, and paths to suit your experiment.

1. **Initial setup**
   - Direct:  
     ```bash
     cd /mnt/storage/tier2/morwur/Projects/DAMIANO/SpeDiction/meg-masc
     export ANALYSIS_NAME=lexical_mem_fullrun_$(date +%Y%m%d_%H%M%S)
     ```
   - SGE note: propagate the variable with `qsub -v ANALYSIS_NAME="$ANALYSIS_NAME" …` (or add `#$ -v ANALYSIS_NAME` to the script header). SGE does not support a `-ANALYSIS_NAME` flag.

2. **A1 – Preprocess raw MEG runs**
   - Direct: `python A1_preprocess_data.py --subjects 01 02 --overwrite`
   - Output: `derivatives/preprocessed/sub-XX/ses-*/task-*/…`
   - SGE: `qsub -v ANALYSIS_NAME="$ANALYSIS_NAME" -t 1-27 s2_submit_A1_preproc.sh -- --overwrite`

3. **A2 – Concatenate derivatives**
   - Direct: `python A2_concatenate_subject_data.py --subject sub-01 --overwrite`
   - Output: `derivatives/preprocessed/sub-XX/concatenated/*_concatenated_*`
   - SGE: `qsub -v ANALYSIS_NAME="$ANALYSIS_NAME" -t 1-27 s2_submit_A2_concat.sh -- --overwrite`

4. **A3 – Resample concatenates (100 Hz)**
   - Direct: `python A3_resample_concatenated_data.py --subject sub-01 --target-rate 100 --overwrite`
   - Output: 100 Hz MEG/mask arrays in the concatenated directory
   - SGE: `qsub -v ANALYSIS_NAME="$ANALYSIS_NAME" -t 1-27 s2_submit_A3_resample.sh -- --target-rate 100 --overwrite`

5. **B1 – Speech envelopes**
   - Direct: `python B1_model_envelope.py --subjects 01 02 --target-rate 100 --overwrite`
   - Output: `derivatives/Models/envelope/sub-XX/...`

6. **B2 – Word frequency trajectories**
   - Direct: `python B2_wordfreq.py --subjects 01 02 --target-rate 100 --overwrite`
   - Output: `derivatives/Models/wordfreq/sub-XX/...`

7. **B3 – Phoneme voicing**
   - Direct: `python B3_voicing.py --subjects 01 02 --target-rate 100 --overwrite`
   - Output: `derivatives/Models/voicing/sub-XX/...`

8. **B4 – GloVe embeddings**
   - Direct:  
     ```bash
     python B4_glove.py \
       --subjects 01 02 \
       --glove-path /path/to/glove.6B.300d.txt \
       --target-rate 100 \
       --overwrite
     ```
   - Output: `derivatives/Models/glove/sub-XX/...` (100 Hz arrays and norms)

9. **C1 – Subject-level dRSA**
   - Direct: `python C1_dRSA_run.py sub-01 --analysis-name "$ANALYSIS_NAME"`
   - Output: `results/$ANALYSIS_NAME/single_subjects/*` (matrices, metadata, caches, plots)
   - SGE:  
     ```bash
     qsub -v ANALYSIS_NAME="$ANALYSIS_NAME" \
          -t 1-27 \
          s2_submit_C1_subject.sh \
          -- --analysis-name "$ANALYSIS_NAME" --lock-subsample-to-word-onset --allow-overlap
      
      qsub -v ANALYSIS_NAME=first_correlation_nolock_nooverlap \ # pass directly the analysis name
          -t 1 \
          s2_submit_C1_subject.sh
     ```

10. **C2 – Subject plots**
    - Direct: `python C2_plot_dRSA.py sub-01 --analysis-name "$ANALYSIS_NAME"`
    - Output: refreshed PNG plots in `results/$ANALYSIS_NAME/single_subjects/`
    - SGE: `qsub -v ANALYSIS_NAME="$ANALYSIS_NAME" -t 1-27 s2_submit_C2_plot.sh -- --analysis-name "$ANALYSIS_NAME"`

11. **D1 – Group cluster analysis**
    - Direct:  
      ```bash
      python D1_group_cluster_analysis.py \
        --analysis-name "$ANALYSIS_NAME" \
        --subjects $(seq -w 1 27) \
        --models "Envelope" "Phoneme Voicing" "Word Frequency" "GloVe" "GloVe Norm"
      ```
      ```bash
      e.g.:
      python D1_group_cluster_analysis.py --analysis-name "first_correlation_nolock_nooverlap" --subjects $(seq -w 1 27) --models "Envelope" "Phoneme Voicing" "Word Frequency" "GloVe" "GloVe Norm"
      ```
    - Output: `results/$ANALYSIS_NAME/group_level/*` (PNG, PDF, NPZ)
    - SGE: 
      - `qsub -v ANALYSIS_NAME="$ANALYSIS_NAME" s2_submit_D1_group.sh --subjects $(seq -w 1 27)`
      - e.g.: `qsub -v ANALYSIS_NAME=first_correlation_nolock_nooverlap s2_submit_D1_group.sh --subjects $(seq -w 1 27)`
      - Omit the `--` separator when calling this helper; it forwards arguments directly to the Python command.

12. **Wrapper alternatives (optional)**
    - Direct (full pipeline):  
      ```bash
      python pipeline_wrapper.py \
        --subjects 01 02 \
        --glove-path /path/to/glove.6B.300d.txt \
        --analysis-name "$ANALYSIS_NAME" \
        --overwrite
      ```
    - Direct (low storage):  
      ```bash
      python pipeline_wrapper_low_storage.py \
        --subjects 01 02 \
        --glove-path /path/to/glove.6B.300d.txt \
        --analysis-name "$ANALYSIS_NAME" \
        --continue-on-error
      ```
    - SGE:  
      - `qsub -v ANALYSIS_NAME="$ANALYSIS_NAME" s2_submit_python_wrapper.sh -- --subjects 01 02 --glove-path /path/to/glove.6B.300d.txt --analysis-name "$ANALYSIS_NAME" --overwrite`  
      - `qsub -v ANALYSIS_NAME="$ANALYSIS_NAME" s2_submit_python_wrapper_low_storage.sh -- --subjects 01-27 --glove-path /path/to/glove.6B.300d.txt --analysis-name "$ANALYSIS_NAME" --continue-on-error --keep-reports`

When launching any qsub job, always forward the analysis name with `-v ANALYSIS_NAME=…` so every node writes into the same `results/<analysis_name>/` tree.

Happy analysing!
