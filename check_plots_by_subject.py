#!/usr/bin/env python3
"""
Visualise stored dRSA lag curves per model/participant using cached group summaries.

This script loads the ``group_dRSA_summary*.npz`` caches generated by
``D1_group_cluster_analysis.py`` and renders compact figures containing one panel
per participant. Each subplot shows the subject lag curve, an optional model
autocorrelation overlay (if ``--overlay-autocorrelation`` is provided), the group
average reference trace, and the peak positions of any significant clusters
recorded in the cache.

It requires both the analysis name (to locate the caches) and a target plot folder
inside ``results/`` where the new figures will be written.
"""

from __future__ import annotations

import argparse
import logging
import math
import os
from pathlib import Path
from typing import Any, Dict, List, Optional, Sequence, Tuple

import matplotlib.pyplot as plt
import numpy as np

from functions.generic_helpers import read_repository_root


LOGGER = logging.getLogger(__name__)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=(
            "Render per-participant lag plots for a single model using cached group summaries."
        )
    )
    parser.add_argument(
        "--analysis-name",
        required=True,
        help="Name of the analysis inside results/ providing cached group outputs.",
    )
    parser.add_argument(
        "--plot-location",
        required=True,
        help="Subfolder name inside results/ where regenerated plots will be stored.",
    )
    parser.add_argument(
        "--model",
        required=True,
        help="Exact model label (matching the cached metadata) to visualise.",
    )
    parser.add_argument(
        "--results-root",
        type=Path,
        default=Path("results"),
        help="Root directory containing analysis folders (default: results/ relative to the repo).",
    )
    parser.add_argument(
        "--n-columns",
        type=int,
        default=4,
        help="Number of subplot columns for participant panels (default: 4).",
    )
    parser.add_argument(
        "--x-tick-interval",
        type=float,
        default=0.5,
        help="Spacing in seconds between x-axis ticks (default: 0.5, i.e., 500 ms).",
    )
    parser.add_argument(
        "--overlay-autocorrelation",
        action="store_true",
        help=(
            "Overlay the model's autocorrelation curve from simulation outputs on every subplot. "
            "Requires simulation lag curves to be present under results/<analysis>/simulations."
        ),
    )
    parser.add_argument(
        "--autocorr-ylim",
        nargs=2,
        type=float,
        metavar=("YMIN", "YMAX"),
        help=(
            "When provided together with --overlay-autocorrelation, draws the autocorrelation "
            "curve on its own secondary axis constrained to these limits."
        ),
    )
    parser.add_argument(
        "--log-level",
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        help="Verbosity level for console logging.",
    )
    return parser.parse_args()


def _coerce_np_payload(value: Any) -> Any:
    if isinstance(value, np.ndarray):
        try:
            value = value.item()
        except ValueError:
            value = value.tolist()
    if isinstance(value, list) and len(value) == 1 and isinstance(value[0], dict):
        return value[0]
    return value


def _extract_optional_str(value: Any) -> Optional[str]:
    if value is None:
        return None
    if isinstance(value, np.ndarray):
        try:
            value = value.item()
        except ValueError:
            value = value.tolist()
    return None if value is None else str(value)


def _load_cache_entries(cache_paths: Sequence[Path]) -> List[Dict[str, Any]]:
    entries: List[Dict[str, Any]] = []
    for cache_path in cache_paths:
        with np.load(cache_path, allow_pickle=True) as payload:
            model_labels = payload["model_labels"].tolist()
            neural_label = _extract_optional_str(payload.get("neural_label"))
            analysis_caption = payload.get("analysis_caption")
            if isinstance(analysis_caption, np.ndarray):
                try:
                    analysis_caption = analysis_caption.item()
                except ValueError:
                    analysis_caption = analysis_caption.tolist()
            analysis_settings = _coerce_np_payload(payload.get("analysis_settings"))
            subject_lag_curves = payload.get("subject_lag_curves")
            if subject_lag_curves is None:
                LOGGER.warning(
                    "Cache %s lacks subject lag curves; skipping this entry.",
                    cache_path,
                )
                continue
            subject_lag_curves = np.asarray(subject_lag_curves)
            if subject_lag_curves.ndim != 3:
                LOGGER.warning(
                    "Subject lag curves in %s had unexpected shape %s; skipping this entry.",
                    cache_path,
                    subject_lag_curves.shape,
                )
                continue
            subjects = payload.get("subjects")
            if isinstance(subjects, np.ndarray):
                subjects = subjects.tolist()
            entry = {
                "cache_path": cache_path,
                "avg_lag_curves": payload["avg_lag_curves"],
                "sem_lag_curves": payload["sem_lag_curves"],
                "lags_sec": payload["lags_sec"],
                "model_labels": model_labels,
                "neural_label": neural_label,
                "analysis_caption": analysis_caption,
                "analysis_settings": analysis_settings,
                "subject_lag_curves": subject_lag_curves,
                "lag_significance_masks": payload["significance_masks"],
                "subjects": subjects,
            }
            entries.append(entry)
    return entries


def _label_to_sim_fragment(label: str) -> str:
    fragment = str(label).replace(" ", "_")
    fragment = fragment.replace(os.sep, "_").replace("/", "_").strip("_")
    return fragment or "model"


def _load_model_autocorr_curve(
    simulations_dir: Path,
    lag_metric: str,
    model_label: str,
    model_index: int,
    expected_length: int,
) -> Optional[np.ndarray]:
    if not simulations_dir.exists():
        LOGGER.warning(
            "Simulations directory %s not found; cannot overlay autocorrelation.",
            simulations_dir,
        )
        return None
    fragment = _label_to_sim_fragment(model_label)
    pattern = f"sub-*_res100_{lag_metric}_sim_*_{fragment}_lag_curves.npy"
    candidates = sorted(simulations_dir.glob(pattern))
    if not candidates:
        LOGGER.warning(
            "No simulation lag curves matching %s for model '%s'.",
            pattern,
            model_label,
        )
        return None
    for npy_path in candidates:
        try:
            data = np.load(npy_path)
        except Exception as exc:
            LOGGER.warning("Failed to load %s: %s", npy_path, exc)
            continue
        if data.ndim == 4:
            if model_index >= data.shape[1]:
                LOGGER.warning(
                    "Simulation file %s lacks model index %d (shape=%s).",
                    npy_path,
                    model_index,
                    data.shape,
                )
                continue
            curves = data[:, model_index, :, :]
        elif data.ndim == 3:
            if model_index >= data.shape[0]:
                LOGGER.warning(
                    "Simulation file %s lacks model index %d (shape=%s).",
                    npy_path,
                    model_index,
                    data.shape,
                )
                continue
            curves = data[model_index][None, ...]
        else:
            LOGGER.warning(
                "Simulation file %s has unsupported shape %s; skipping.",
                npy_path,
                data.shape,
            )
            continue
        if curves.shape[-1] != expected_length:
            LOGGER.warning(
                "Simulation file %s provides %d lag samples, expected %d; skipping.",
                npy_path,
                curves.shape[-1],
                expected_length,
            )
            continue
        merged = curves.reshape(-1, curves.shape[-1])
        return merged.mean(axis=0)
    LOGGER.warning(
        "Unable to derive autocorrelation curve for model '%s' using %s.",
        model_label,
        simulations_dir,
    )
    return None


def _resolve_model_index(model_labels: Sequence[str], model_name: str) -> int:
    if model_name not in model_labels:
        raise ValueError(
            f"Requested model '{model_name}' not found in cache labels: {model_labels}"
        )
    return model_labels.index(model_name)


def _compute_y_limits(
    subject_curves: np.ndarray,
    avg_curve: np.ndarray,
    sem_curve: np.ndarray,
    padding: float = 0.02,
) -> Tuple[float, float]:
    min_val = min(subject_curves.min(), (avg_curve - sem_curve).min())
    max_val = max(subject_curves.max(), (avg_curve + sem_curve).max())
    if math.isfinite(min_val) and math.isfinite(max_val):
        span = max_val - min_val
        if span <= 0:
            span = abs(max_val) or 1.0
        min_val -= padding * span
        max_val += padding * span
    return float(min_val), float(max_val)


def _find_cluster_peaks(
    significance_mask: np.ndarray, avg_curve: np.ndarray, lags_sec: np.ndarray
) -> List[float]:
    """Return lag positions corresponding to the strongest point within each contiguous cluster."""
    if significance_mask is None or not np.any(significance_mask):
        return []
    indices = np.flatnonzero(significance_mask)
    if not indices.size:
        return []
    split_points = np.where(np.diff(indices) > 1)[0] + 1
    clusters = np.split(indices, split_points)
    peaks: List[float] = []
    for cluster in clusters:
        cluster_curve = avg_curve[cluster]
        peak_offset = int(np.argmax(np.abs(cluster_curve)))
        peak_idx = cluster[peak_offset]
        peaks.append(float(lags_sec[peak_idx]))
    return peaks


def _build_fig(
    lags_sec: np.ndarray,
    avg_curve: np.ndarray,
    sem_curve: np.ndarray,
    subject_curves: np.ndarray,
    subject_ids: Sequence[str],
    model_name: str,
    title_suffix: str,
    n_columns: int,
    x_tick_interval: float,
    cluster_peaks: Sequence[float],
    autocorr_curve: Optional[np.ndarray],
    autocorr_ylim: Optional[Tuple[float, float]],
) -> plt.Figure:
    n_subjects = subject_curves.shape[0]
    n_rows = math.ceil(n_subjects / n_columns)
    fig_height = 2.0 * n_rows
    fig = plt.figure(figsize=(14, fig_height))
    gs = fig.add_gridspec(n_rows, n_columns)
    y_min, y_max = _compute_y_limits(subject_curves, avg_curve, sem_curve)

    tick_start = math.floor(lags_sec[0] / x_tick_interval) * x_tick_interval
    tick_end = lags_sec[-1] + (x_tick_interval * 0.5)
    tick_positions = np.arange(tick_start, tick_end, x_tick_interval)
    overlay_color = "#bbbbbb"
    for idx, subject_id in enumerate(subject_ids):
        row = idx // n_columns
        col = idx % n_columns
        ax = fig.add_subplot(gs[row, col])
        ax.plot(lags_sec, avg_curve, color=overlay_color, linewidth=0.8, label="_nolegend_")
        ax.plot(lags_sec, subject_curves[idx], color="#2ca02c", linewidth=1.0)
        if autocorr_curve is not None:
            target_ax = ax
            if autocorr_ylim is not None:
                target_ax = ax.twinx()
                target_ax.set_ylim(*autocorr_ylim)
                target_ax.set_ylabel("Autocorr", color="#ff7f0e")
                target_ax.tick_params(axis="y", labelcolor="#ff7f0e")
            target_ax.plot(
                lags_sec,
                autocorr_curve,
                color="#ff7f0e",
                linestyle="--",
                linewidth=0.9,
            )
        for peak_lag in cluster_peaks:
            ax.axvline(
                peak_lag,
                color=overlay_color,
                linewidth=0.8,
                alpha=0.8,
                linestyle="-",
            )
        ax.axvline(0, color="k", linestyle="--", linewidth=0.5)
        ax.axhline(0, color="k", linestyle="--", linewidth=0.5)
        ax.set_xlim(lags_sec[0], lags_sec[-1])
        ax.set_ylim(y_min, y_max)
        ax.set_xticks(tick_positions)
        ax.set_xticklabels([f"{tick:.2f}" for tick in tick_positions], rotation=45, fontsize=8)
        ax.set_title(f"Subject {subject_id}", fontsize=9)
        if col == 0:
            ax.set_ylabel("Correlation")
        if row == n_rows - 1:
            ax.set_xlabel("Lag (s)")
    fig.tight_layout()
    return fig


def main() -> int:
    args = parse_args()
    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format="%(asctime)s - %(levelname)s - %(message)s",
    )

    repo_root = read_repository_root()
    results_root = args.results_root
    if not results_root.is_absolute():
        results_root = (repo_root / results_root).resolve()

    analysis_dir = (results_root / args.analysis_name).resolve()
    if not analysis_dir.exists():
        raise FileNotFoundError(
            f"Analysis '{args.analysis_name}' not found under {results_root}."
        )

    group_level_dir = analysis_dir / "group_level"
    if not group_level_dir.exists():
        raise FileNotFoundError(
            f"Group-level directory missing at {group_level_dir}. Run the group analysis first."
        )

    plot_location = Path(args.plot_location)
    if plot_location.is_absolute():
        target_dir = plot_location
    else:
        target_dir = (results_root / plot_location).resolve()
    target_dir.mkdir(parents=True, exist_ok=True)

    cache_paths = sorted(group_level_dir.glob("group_dRSA_summary*.npz"))
    if not cache_paths:
        raise FileNotFoundError(
            f"No cached group summaries found in {group_level_dir}. "
            "Ensure D1_group_cluster_analysis.py was run with caching enabled."
        )

    cache_entries = _load_cache_entries(cache_paths)
    if not cache_entries:
        raise RuntimeError(
            "No caches contained per-subject lag curves; rerun the group analysis with the overlay flag."
        )

    if args.autocorr_ylim and not args.overlay_autocorrelation:
        raise ValueError("--autocorr-ylim requires --overlay-autocorrelation.")
    autocorr_ylim: Optional[Tuple[float, float]] = None
    if args.autocorr_ylim:
        ymin, ymax = args.autocorr_ylim
        if ymin >= ymax:
            raise ValueError("--autocorr-ylim expects ymin < ymax.")
        autocorr_ylim = (ymin, ymax)

    simulations_dir = analysis_dir / "simulations"
    autocorr_cache: Dict[Tuple[str, str, int], Optional[np.ndarray]] = {}

    for entry in cache_entries:
        model_idx = _resolve_model_index(entry["model_labels"], args.model)
        subject_curves = entry["subject_lag_curves"][:, model_idx, :]
        avg_curve = entry["avg_lag_curves"][model_idx]
        sem_curve = entry["sem_lag_curves"][model_idx]
        lags_sec = entry["lags_sec"]
        subjects = entry.get("subjects") or []
        if not subjects:
            subjects = [f"{idx+1:02d}" for idx in range(subject_curves.shape[0])]
        significance_mask = entry["lag_significance_masks"][model_idx]
        cluster_peaks = _find_cluster_peaks(significance_mask, avg_curve, lags_sec)
        autocorr_curve = None
        if args.overlay_autocorrelation:
            lag_metric = "correlation"
            settings = entry.get("analysis_settings")
            if isinstance(settings, dict):
                lag_metric = settings.get("lag_metric", lag_metric)
            cache_key = (args.model, lag_metric, subject_curves.shape[-1])
            if cache_key not in autocorr_cache:
                autocorr_cache[cache_key] = _load_model_autocorr_curve(
                    simulations_dir,
                    lag_metric,
                    args.model,
                    model_idx,
                    subject_curves.shape[-1],
                )
            autocorr_curve = autocorr_cache[cache_key]

        fig = _build_fig(
            lags_sec=lags_sec,
            avg_curve=avg_curve,
            sem_curve=sem_curve,
            subject_curves=subject_curves,
            subject_ids=subjects,
            model_name=args.model,
            title_suffix=entry["neural_label"] or entry["cache_path"].stem,
            n_columns=args.n_columns,
            x_tick_interval=args.x_tick_interval,
            cluster_peaks=cluster_peaks,
            autocorr_curve=autocorr_curve,
            autocorr_ylim=autocorr_ylim,
        )
        extra_suffix = ""
        if autocorr_ylim:
            extra_suffix = f"_autocorr_{autocorr_ylim[0]:.2f}_{autocorr_ylim[1]:.2f}"
            extra_suffix = extra_suffix.replace(".", "p").replace("-", "m")
        filename = f"{entry['cache_path'].stem}_{args.model}_lag_panels{extra_suffix}"
        output_path = target_dir / f"{filename}.png"
        fig.savefig(output_path, dpi=200)
        plt.close(fig)
        LOGGER.info("Saved lag panel figure to %s.", output_path)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
