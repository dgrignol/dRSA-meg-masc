#!/usr/bin/env python3
"""
Build concatenated Transformer (GPT-2 / BERT) embedding trajectories aligned with 100 Hz MEG timelines.

This script mirrors the behaviour of B4_glove.py but uses Hugging Face
Transformers to compute contextualised token embeddings for each stimulus word.
For each requested model (default: gpt2, bert-base-uncased) and subject it:

1. Reads the concatenation metadata to recover the ordered list of (session, task) runs.
2. Loads the corresponding BIDS events to extract word onsets/durations.
3. Computes per-word embeddings using the specified Transformer and pools
   subword pieces by averaging the last hidden layer across non-special tokens.
4. Allocates a memory-mapped array shaped (hidden_size, timepoints_100Hz) and
   fills it with random embeddings sampled from the vocabulary for every time point with no word.
5. Overwrites the segments covered by each word with the appropriate embedding.

Outputs (per subject, per model):
- ``*_concatenated_<model>_100Hz.npy`` (float32 memmap-compatible array)
- ``*_concatenated_<model>_100Hz_norm.npy`` with L2 norm over time
- ``*_concatenated_<model>_100Hz_metadata.json`` with provenance and summary stats
- Optional ``*_concatenated_<model>_100Hz_plot.png`` showing a heatmap + L2 norm

Dependencies (install if missing):
- PyTorch:    pip install torch
- Transformers: pip install transformers

Model weights (downloaded automatically on first run):
- GPT-2:             "gpt2"
- BERT base uncased: "bert-base-uncased"

The repository root is resolved via data_path.txt; preprocessed MEG concatenation
metadata and events must already exist (generated by A1–A3 scripts).
"""

from __future__ import annotations

import argparse
import ast
import csv
import json
import logging
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

import numpy as np
from numpy.lib.format import open_memmap
import matplotlib

matplotlib.use("Agg")
from matplotlib import pyplot as plt

from functions.generic_helpers import read_repository_root

LOGGER = logging.getLogger(__name__)


# ---------------------------------------------------------------------------
# Data classes
# ---------------------------------------------------------------------------


@dataclass(frozen=True)
class RunDescriptor:
    subject: str
    session: str
    task: str
    metadata_path: Path
    events_path: Path
    sfreq: float
    n_samples: int


@dataclass
class WordEvent:
    word: str
    sample: int
    duration: float


# ---------------------------------------------------------------------------
# Utilities shared with B4_glove-style processing
# ---------------------------------------------------------------------------


def normalise_subject(label: str) -> str:
    label = label.strip()
    if label.startswith("sub-"):
        return label
    return f"sub-{int(label):02d}"


def normalise_session(label: str) -> str:
    label = label.strip()
    if label.startswith("ses-"):
        return label
    return f"ses-{label}"


def normalise_task(label: str) -> str:
    label = label.strip()
    if label.startswith("task-"):
        return label
    return f"task-{label}"


def iter_preprocessed_runs(
    preproc_root: Path,
    bids_root: Path,
    subjects: Optional[Sequence[str]],
) -> Iterable[RunDescriptor]:
    subject_filter = None
    if subjects:
        subject_filter = {normalise_subject(s) for s in subjects}

    for subj_dir in sorted(preproc_root.glob("sub-*")):
        subject = subj_dir.name
        if subject_filter and subject not in subject_filter:
            continue

        for meta_path in sorted(subj_dir.glob("ses-*/task-*/sub-*_metadata.json")):
            with meta_path.open("r") as fh:
                meta = json.load(fh)

            session = normalise_session(meta["session"])
            task = normalise_task(meta["task"])
            events_path = (
                bids_root
                / subject
                / session
                / "meg"
                / f"{subject}_{session}_{task}_events.tsv"
            )

            if not events_path.exists():
                LOGGER.warning("Events TSV missing for %s; skipping.", meta_path)
                continue

            sfreq = float(meta["sfreq"])
            n_samples = int(meta["n_samples"])
            yield RunDescriptor(
                subject=subject,
                session=session,
                task=task,
                metadata_path=meta_path,
                events_path=events_path,
                sfreq=sfreq,
                n_samples=n_samples,
            )


def parse_trial_info(value: str, events_path: Path) -> Optional[dict]:
    try:
        return ast.literal_eval(value)
    except (SyntaxError, ValueError):
        LOGGER.debug("Unable to parse trial_type in %s", events_path)
        return None


def load_word_events(events_path: Path) -> List[WordEvent]:
    events: List[WordEvent] = []
    with events_path.open("r", encoding="utf-8-sig", newline="") as handle:
        reader = csv.DictReader(handle, delimiter="\t")
        for row in reader:
            trial_info = parse_trial_info(row["trial_type"], events_path)
            if not trial_info or trial_info.get("kind") != "word":
                continue

            if float(trial_info.get("pronounced", 1.0)) == 0.0:
                continue

            try:
                onset_sample = int(row["sample"])
                duration = float(row["duration"])
            except (TypeError, ValueError):
                continue

            word = str(trial_info.get("word", "")).strip()
            if not word:
                continue
            events.append(WordEvent(word=word, sample=onset_sample, duration=duration))
    return events


def normalise_word_token(word: str) -> str:
    token = word.lower().strip()
    token = token.replace("’", "'")
    return token


def generate_word_candidates(word: str) -> List[str]:
    base = normalise_word_token(word)
    candidates = [base]
    stripped = base.strip(".,!?;:\"()[]{}")
    if stripped != base:
        candidates.append(stripped)
    if base.replace("-", "") != base:
        candidates.append(base.replace("-", ""))
    if base.replace("'", "") != base:
        candidates.append(base.replace("'", ""))
    if base.replace("’", "") != base:
        candidates.append(base.replace("’", ""))
    return list(dict.fromkeys(candidates))


def collect_vocabulary(
    events_iterable: Iterable[List[WordEvent]],
) -> List[str]:
    vocab = set()
    for events in events_iterable:
        for event in events:
            for candidate in generate_word_candidates(event.word):
                if candidate:
                    vocab.add(candidate)
    return sorted(vocab)


def compute_segment_length(descriptor: RunDescriptor, target_rate: float) -> int:
    return int(round(descriptor.n_samples * target_rate / descriptor.sfreq))


def allocate_subject_memmap(
    output_path: Path,
    embedding_dim: int,
    total_samples: int,
) -> np.memmap:
    output_path.parent.mkdir(parents=True, exist_ok=True)
    mm = open_memmap(
        str(output_path),
        mode="w+",
        dtype=np.float32,
        shape=(embedding_dim, total_samples),
        fortran_order=False,
    )
    return mm


def fill_with_random_embeddings(
    target: np.memmap,
    start: int,
    stop: int,
    embedding_pool: np.ndarray,
    rng: np.random.Generator,
    chunk_size: int = 50000,
) -> None:
    length = stop - start
    if length <= 0:
        return

    pool_size = embedding_pool.shape[0]
    pos = 0
    while pos < length:
        block = min(chunk_size, length - pos)
        idx = rng.integers(0, pool_size, size=block)
        target[:, start + pos : start + pos + block] = embedding_pool[idx].T
        pos += block


def assign_word_embedding(
    target: np.memmap,
    start: int,
    stop: int,
    embedding: np.ndarray,
) -> None:
    if stop <= start:
        return
    target[:, start:stop] = embedding[:, None]


def compute_l2_norm_series(
    data: np.memmap,
    chunk_size: int = 50000,
) -> np.ndarray:
    total = data.shape[1]
    norms = np.empty(total, dtype=np.float32)
    for start in range(0, total, chunk_size):
        stop = min(total, start + chunk_size)
        block = np.asarray(data[:, start:stop], dtype=np.float32)
        norms[start:stop] = np.linalg.norm(block, axis=0)
    return norms


def plot_embedding_summary(
    data: np.ndarray,
    norms: np.ndarray,
    sfreq: float,
    output_path: Path,
    max_points: int,
    model_label: str,
) -> Path:
    if max_points <= 0:
        raise ValueError("max_points must be positive.")

    n_timepoints = data.shape[1]
    step = max(1, int(np.ceil(n_timepoints / max_points)))
    indices = np.arange(0, n_timepoints, step, dtype=int)
    sampled = np.asarray(data[:, indices], dtype=np.float32)
    t_axis = indices / float(sfreq)
    norm_samples = norms[indices]

    fig, (ax_top, ax_bottom) = plt.subplots(
        2, 1, figsize=(10, 6), sharex=True, gridspec_kw={"height_ratios": [3, 1]}
    )

    im = ax_top.imshow(
        sampled,
        aspect="auto",
        origin="lower",
        extent=[t_axis[0], t_axis[-1] if len(t_axis) > 1 else 0.0, 0, sampled.shape[0]],
        cmap="viridis",
    )
    ax_top.set_ylabel("Hidden size")
    ax_top.set_title(f"{model_label} trajectory (subsampled heatmap)")
    fig.colorbar(im, ax=ax_top, fraction=0.046, pad=0.04, label="Value")

    ax_bottom.plot(t_axis, norm_samples, linewidth=0.8, color="tab:green")
    ax_bottom.set_xlabel("Time (s)")
    ax_bottom.set_ylabel("L2 norm")
    ax_bottom.set_title("Embedding L2 norm over time")
    ax_bottom.grid(alpha=0.3)

    fig.tight_layout(rect=[0, 0.08, 1, 0.98])
    fig.subplots_adjust(hspace=0.35)
    fig.text(0.5, 0.03, model_label, ha="center", va="center", fontsize=8)

    fig.savefig(output_path, dpi=200)
    plt.close(fig)
    return output_path


# ---------------------------------------------------------------------------
# Transformer-specific embedding extraction
# ---------------------------------------------------------------------------


def _import_transformers_stack():
    try:
        import torch
        from transformers import AutoModel, AutoTokenizer
    except Exception as exc:  # pragma: no cover - import-time guard
        raise RuntimeError(
            "Missing dependencies for transformer embeddings. Install with:\n"
            "  pip install torch transformers\n"
            f"Original error: {exc}"
        ) from exc
    return torch, AutoModel, AutoTokenizer


def _select_device(device_arg: str):
    torch, _, _ = _import_transformers_stack()
    if device_arg == "auto":
        return "cuda" if torch.cuda.is_available() else "cpu"
    if device_arg in {"cpu", "cuda"}:
        if device_arg == "cuda" and not torch.cuda.is_available():
            LOGGER.warning("CUDA requested but not available; falling back to CPU.")
            return "cpu"
        return device_arg
    LOGGER.warning("Unknown device '%s'; using CPU.", device_arg)
    return "cpu"


def load_transformer_model(model_id: str, device: str):
    torch, AutoModel, AutoTokenizer = _import_transformers_stack()
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModel.from_pretrained(model_id)
    model.eval()
    model.to(device)

    # Some tokenizers (e.g., GPT-2) lack a pad token by default; use EOS for padding.
    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:
        tokenizer.pad_token = tokenizer.eos_token

    hidden_size = int(getattr(model.config, "hidden_size", model.config.n_embd))
    all_special_ids = set(getattr(tokenizer, "all_special_ids", []) or [])
    return model, tokenizer, hidden_size, all_special_ids


def batch_encode_words(
    model_id: str,
    words: Sequence[str],
    batch_size: int,
    device: str,
) -> Dict[str, np.ndarray]:
    """Return mapping word -> pooled embedding (float32, shape [hidden])."""
    torch, AutoModel, AutoTokenizer = _import_transformers_stack()
    model, tokenizer, hidden_size, all_special_ids = load_transformer_model(model_id, device)

    results: Dict[str, np.ndarray] = {}
    # Work in mini-batches
    with torch.no_grad():
        for start in range(0, len(words), batch_size):
            batch_words = list(words[start : start + batch_size])
            enc = tokenizer(
                batch_words,
                padding=True,
                truncation=True,
                return_tensors="pt",
                return_special_tokens_mask=True,
            )
            enc = {k: v.to(device) for k, v in enc.items()}
            outputs = model(**enc)
            last_hidden = outputs.last_hidden_state  # [B, T, H]
            attn = enc.get("attention_mask", None)
            spm = enc.get("special_tokens_mask", None)

            # Build mask of valid (non-special, attended) tokens per sequence
            if spm is not None:
                valid_mask = (spm == 0)
            else:
                # Fallback: assume all attended tokens are valid
                valid_mask = attn.bool() if attn is not None else torch.ones(last_hidden.shape[:2], dtype=torch.bool, device=last_hidden.device)

            if attn is not None:
                valid_mask = valid_mask & attn.bool()

            # Compute mean over valid tokens per row
            # Avoid empty selections by falling back to mean over attended tokens
            B, T, H = last_hidden.shape
            for i, word in enumerate(batch_words):
                mask_i = valid_mask[i]
                if not torch.any(mask_i):
                    mask_i = attn[i].bool() if attn is not None else torch.ones(T, dtype=torch.bool, device=last_hidden.device)
                vec = last_hidden[i, mask_i, :].mean(dim=0)
                results[word] = vec.detach().cpu().to(torch.float32).numpy()

    # Safety: ensure consistent dimensionality
    for w, v in list(results.items()):
        if v.shape != (hidden_size,):
            results[w] = np.asarray(v).reshape(-1).astype(np.float32)
    return results


def ensure_embeddings_matrix(embeddings: Dict[str, np.ndarray]) -> np.ndarray:
    if not embeddings:
        raise ValueError("No embeddings available to populate the trajectory.")
    matrix = np.vstack(list(embeddings.values())).astype(np.float32, copy=False)
    return matrix


def lookup_embedding(
    word: str,
    embeddings: Dict[str, np.ndarray],
    fallback_pool: np.ndarray,
    rng: np.random.Generator,
    missing_stats: Dict[str, int],
) -> np.ndarray:
    for candidate in generate_word_candidates(word):
        emb = embeddings.get(candidate)
        if emb is not None:
            return emb
    missing_stats["words_missing"] += 1
    idx = rng.integers(0, fallback_pool.shape[0])
    return fallback_pool[idx]


def prepare_subject_segments(
    subject: str,
    descriptors_by_key: Dict[Tuple[str, str], RunDescriptor],
    preproc_root: Path,
) -> List[RunDescriptor]:
    concat_meta_path = (
        preproc_root / subject / "concatenated" / f"{subject}_concatenation_metadata.json"
    )
    if not concat_meta_path.exists():
        LOGGER.warning("Concatenation metadata missing for %s; skipping subject.", subject)
        return []

    concat_meta = json.loads(concat_meta_path.read_text())
    ordered_descriptors: List[RunDescriptor] = []
    for segment in concat_meta.get("segments", []):
        session = segment["session"]
        task = segment["task"]
        key = (session, task)
        descriptor = descriptors_by_key.get(key)
        if descriptor is None:
            LOGGER.warning(
                "Missing descriptor for %s %s %s; segment skipped.",
                subject,
                session,
                task,
            )
            continue
        ordered_descriptors.append(descriptor)

    return ordered_descriptors


def build_subject_transformer(
    subject: str,
    ordered_descriptors: List[RunDescriptor],
    word_events: Dict[Tuple[str, str], List[WordEvent]],
    embeddings: Dict[str, np.ndarray],
    embedding_matrix: np.ndarray,
    embedding_dim: int,
    models_root: Path,
    model_label: str,
    target_rate: float,
    rng: np.random.Generator,
    random_seed: int,
    overwrite: bool,
    plot: bool,
    plot_max_points: int,
) -> None:
    if not ordered_descriptors:
        LOGGER.warning("No valid descriptors for %s; skipping.", subject)
        return

    total_samples = sum(
        compute_segment_length(descriptor, target_rate) for descriptor in ordered_descriptors
    )
    subject_dir = models_root / subject / "concatenated"
    subject_dir.mkdir(parents=True, exist_ok=True)
    base_name = f"{subject}_concatenated_{model_label}_{int(target_rate)}Hz"
    array_path = subject_dir / f"{base_name}.npy"
    metadata_path = subject_dir / f"{base_name}_metadata.json"

    if array_path.exists() and not overwrite:
        LOGGER.info("Concatenated %s model exists for %s; skipping.", model_label, subject)
        return

    target_memmap = allocate_subject_memmap(array_path, embedding_dim, total_samples)

    offset = 0
    all_words = 0
    missing_stats = {"words_missing": 0}
    coverage_samples = 0

    for descriptor in ordered_descriptors:
        seg_len = compute_segment_length(descriptor, target_rate)
        seg_start = offset
        seg_end = offset + seg_len

        fill_with_random_embeddings(
            target_memmap,
            seg_start,
            seg_end,
            embedding_matrix,
            rng,
        )

        events = word_events.get((descriptor.session, descriptor.task), [])
        for event in events:
            all_words += 1
            embedding = lookup_embedding(
                event.word,
                embeddings,
                embedding_matrix,
                rng,
                missing_stats,
            )

            onset_sec = event.sample / descriptor.sfreq
            offset_sec = onset_sec + max(event.duration, 0.0)

            start_idx = seg_start + int(np.floor(onset_sec * target_rate))
            end_idx = seg_start + int(np.ceil(offset_sec * target_rate))

            if end_idx <= start_idx:
                end_idx = start_idx + 1
            start_idx = max(seg_start, start_idx)
            end_idx = min(seg_end, end_idx)

            assign_word_embedding(
                target_memmap,
                start_idx,
                end_idx,
                embedding.astype(np.float32, copy=False),
            )
            coverage_samples += max(0, end_idx - start_idx)

        offset += seg_len

    target_memmap.flush()

    norm_vector = compute_l2_norm_series(target_memmap)
    norm_path = subject_dir / f"{base_name}_norm.npy"
    np.save(norm_path, norm_vector.astype(np.float32, copy=False))

    metadata = {
        "subject": subject,
        "embedding_dim": int(embedding_dim),
        "target_rate_hz": float(target_rate),
        "total_timepoints": int(total_samples),
        "words_total": int(all_words),
        "words_missing": int(missing_stats["words_missing"]),
        "coverage_samples": int(coverage_samples),
        "random_seed": int(random_seed),
        "model_label": model_label,
    }
    with metadata_path.open("w") as fh:
        json.dump(metadata, fh, indent=2)

    if plot:
        plot_path = subject_dir / f"{base_name}_plot.png"
        plot_embedding_summary(
            np.asarray(target_memmap, dtype=np.float32),
            norm_vector,
            sfreq=target_rate,
            output_path=plot_path,
            max_points=plot_max_points,
            model_label=model_label,
        )


# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------


def build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description=(
            "Build concatenated Transformer (GPT-2 / BERT) word-embedding trajectories "
            "aligned with 100 Hz MEG timelines."
        )
    )
    parser.add_argument(
        "--subjects",
        nargs="*",
        help="Optional subset of subjects (e.g., 01 02 or sub-01 sub-02). Defaults to all available.",
    )
    parser.add_argument(
        "--models",
        nargs="+",
        default=["gpt2", "bert-base-uncased"],
        help=(
            "Transformer model identifiers (Hugging Face repo IDs). "
            "Examples: gpt2, gpt2-medium, bert-base-uncased, roberta-base"
        ),
    )
    parser.add_argument(
        "--device",
        default="auto",
        choices=["auto", "cpu", "cuda"],
        help="Computation device for embedding extraction (default: auto).",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=128,
        help="Batch size for embedding extraction (default: 128).",
    )
    parser.add_argument(
        "--target-rate",
        type=float,
        default=100.0,
        help="Output sampling rate in Hz (default: 100).",
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="Overwrite existing outputs.",
    )
    parser.add_argument(
        "--random-seed",
        type=int,
        default=0,
        help="Seed controlling the random fill for silent intervals (default: 0).",
    )
    parser.add_argument(
        "--plot",
        action="store_true",
        help="Generate a diagnostic plot (L2 norm over time).",
    )
    parser.add_argument(
        "--plot-max-points",
        type=int,
        default=20000,
        help="Maximum number of samples to render in the plot (default: 20k).",
    )
    parser.add_argument(
        "--log-level",
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        help="Verbosity for console logging.",
    )
    return parser


def main(argv: Optional[Sequence[str]] = None) -> int:
    parser = build_arg_parser()
    args = parser.parse_args(argv)

    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format="%(asctime)s - %(levelname)s - %(message)s",
    )

    repo_root = read_repository_root()
    preproc_root = repo_root / "derivatives" / "preprocessed"
    bids_root = repo_root / "bids_anonym"

    subject_filter = [normalise_subject(s) for s in args.subjects] if args.subjects else None

    descriptors_by_subject: Dict[str, Dict[Tuple[str, str], RunDescriptor]] = defaultdict(dict)
    word_events: Dict[Tuple[str, str, str], List[WordEvent]] = {}

    for descriptor in iter_preprocessed_runs(preproc_root, bids_root, subject_filter):
        descriptors_by_subject[descriptor.subject][(descriptor.session, descriptor.task)] = (
            descriptor
        )
        key = (descriptor.subject, descriptor.session, descriptor.task)
        events = load_word_events(descriptor.events_path)
        word_events[key] = events

    subjects = sorted(descriptors_by_subject.keys())
    if not subjects:
        LOGGER.error("No subjects found matching the criteria.")
        return 1

    if not word_events:
        LOGGER.error("No word events found for the selected subjects.")
        return 1

    vocab = collect_vocabulary(word_events.values())
    if not vocab:
        LOGGER.error("Vocabulary extracted from events is empty; cannot build embeddings.")
        return 1

    device = _select_device(args.device)
    rng = np.random.default_rng(args.random_seed)

    for model_id in args.models:
        LOGGER.info("Preparing embeddings for model: %s (device=%s)", model_id, device)
        # Compute all candidate embeddings once per model
        embeddings = batch_encode_words(model_id, vocab, batch_size=args.batch_size, device=device)
        if not embeddings:
            LOGGER.error("Model %s produced no embeddings for the vocabulary.", model_id)
            return 1
        # Hidden size inferred from first vector
        first_vec = next(iter(embeddings.values()))
        embedding_dim = int(np.asarray(first_vec).size)
        embedding_matrix = ensure_embeddings_matrix(embeddings)

        # Output directory per model
        # Store under derivatives/Models/<model_id>
        safe_model_dir = model_id
        models_root = repo_root / "derivatives" / "Models" / safe_model_dir
        models_root.mkdir(parents=True, exist_ok=True)

        for subject in subjects:
            ordered_descriptors = prepare_subject_segments(
                subject,
                descriptors_by_subject[subject],
                preproc_root,
            )
            subject_events = {
                (desc.session, desc.task): word_events.get((subject, desc.session, desc.task), [])
                for desc in ordered_descriptors
            }
            build_subject_transformer(
                subject=subject,
                ordered_descriptors=ordered_descriptors,
                word_events=subject_events,
                embeddings=embeddings,
                embedding_matrix=embedding_matrix,
                embedding_dim=embedding_dim,
                models_root=models_root,
                model_label=model_id.replace("/", "-").replace(" ", "_"),
                target_rate=args.target_rate,
                rng=rng,
                random_seed=args.random_seed,
                overwrite=args.overwrite,
                plot=args.plot,
                plot_max_points=args.plot_max_points,
            )

    return 0


if __name__ == "__main__":
    raise SystemExit(main())

